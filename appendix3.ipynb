{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **工作备忘**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Refs 待看**\n",
    "\n",
    "+ [How to Use Machine Learning to Color Your Lighting Based on Music Mood](https://hackernoon.com/how-to-use-machine-learning-to-color-your-lighting-based-on-music-mood-bi163u8l)\n",
    "\n",
    "+ [Intro to Audio Analysis: Recognizing Sounds Using Machine Learning](https://hackernoon.com/intro-to-audio-analysis-recognizing-sounds-using-machine-learning-qy2r3ufl)\n",
    "\n",
    "+ [Audio Handling Basics: Process Audio Files In Command-Line or Python](https://hackernoon.com/audio-handling-basics-how-to-process-audio-files-using-python-cli-jo283u3y)\n",
    "\n",
    "+ [A deep learning approach to generating Beat Saber levels ](https://github.com/oxai/deepsaber)\n",
    "\n",
    "## **Scratch 待做**\n",
    "\n",
    "播放器：参考[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2021-03-16**\n",
    "\n",
    "**发现初期没必要分不同种类的笔记本去记录整理资料与笔记，因为中间一直变化，就会有很多 dirty notebook。两个 dirty，一个用来写代码实验，一个像这个用来记录与备忘就够了。相对稳定的代码整理出来作为 clean notebook 保存起来。项目尾声的时候，再根据备忘分类整理或写作就可以了。**\n",
    "\n",
    "**音频信号处理相关的概念关系图，fft转换相关的、声音到音频数字化相关的、音频数据在软硬间之间流动的过程，还有曾经特别晕乎的概念（chunk，buffer，bit depth，sample rate，frequency resolution，bar，bin，band等）**\n",
    "\n",
    "**弃用 pyaudio，改用 sounddevice；之前完成的实时录音绘图等代码之后改用 sounddevice 再做一遍，主要是换用 callback 甚至双线程的方式去做，之前的代码实际运行时时间上很可能存在较大偏差**\n",
    "\n",
    "[sounddevice 官方文档](https://python-sounddevice.readthedocs.io/en/0.4.1/examples.html)中给出的多个例子，就是学习音频处理的好材料，更适切我的需求。\n",
    "\n",
    "\n",
    "**Audio data flow 基本理解了**\n",
    "\n",
    "Matlab 官方文档围绕 AudioToolbox 的内容解释的非常清晰。也可能是我现在的认识加深了，因为有一篇文档我前面也看过了，但是关于数据从输入到分析到输出的整个过程，终于有了基本清晰的认识，buffer 的概念也理解到了。     \n",
    "就是这篇文档[Audio I/O: Buffering, Latency, and Throughput](https://www.mathworks.com/help/audio/gs/audio-io-buffering-latency-and-throughput.html)\n",
    "\n",
    "\n",
    "\n",
    "**cool projects**\n",
    "\n",
    "+ [LONGPLAYER](https://longplayer.org/)\n",
    "\n",
    "+ [Future Library](https://www.futurelibrary.no/#/)\n",
    "\n",
    "\n",
    "**参考**\n",
    "\n",
    "1. [Introduction to Sound Programming with ALSA](https://www.linuxjournal.com/article/6735)\n",
    "\n",
    "2. [Basic concepts behind Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API)\n",
    "\n",
    "3. [Real-time analysis of streaming audio data with Web Audio API](http://ianreah.com/2013/02/28/Real-time-analysis-of-streaming-audio-data-with-Web-Audio-API.html)\n",
    "\n",
    "</br>\n",
    "\n",
    "## **2021-03-15**\n",
    "\n",
    "**从 .wav 读取数据，向 pyaudio stream 写入以播放音乐时，如果将声道数设置为 1，采样率设为 44100 播放出来的音乐是有问题的，因为源文件是双声道采样的，播放时，如果设为单声道，采样率要加倍设为 88200 听起来才正常**\n",
    "\n",
    "**从 .wav 读取数据，然后同时在 pyaudio stream 中写入和读取，即同时播放和录音，是可行的，但是这样录音生成后的音频，响度比原来小。在 Scratch 中也是如此，如果在一个项目中播放音乐，同时使用响度侦测，发现侦测到的响度值比正常偏小，这部分需要做标准化检查，看看标准化之后响度值是否一样**\n",
    "\n",
    "</br>\n",
    "\n",
    "## **2021-03-14**\n",
    "\n",
    "**再看看 ALSA audio buffer 的东西，理解一下基本的过程，不然 Josh 对我提问的回复我还是有些想不明白**\n",
    "\n",
    "## **2021-03-13**\n",
    "\n",
    "**测试 pyaudio 从 stream 中读取一个 chunk 所需的时间，发现有些时候非常短，远小于 0.023s,本以为没读够 1024 samples，看 len(data) 返回的结果也是1024 个数据，就不知道为什么用时那么短了，[已在 github 上提问等回复](https://github.com/makerportal/rpi_i2s/issues/1)**\n",
    "\n",
    "**不想只为使用 timer 而引入 qt 框架，当前找到的办法是依据参考 2 给出的结构和代码，使用两个进程**\n",
    "\n",
    "因为 python 线程问题，看到一些并行运算相关的 python 包：\n",
    "\n",
    "+ [Dask: a flexible library for parallel computing in Python.](https://docs.dask.org/en/latest/)\n",
    "\n",
    "+ [ipyparallel](https://github.com/ipython/ipyparallel)\n",
    "\n",
    "\n",
    "**记得干净首尾 (clean up)**\n",
    "\n",
    "前面 pyaudio stream 开启后，最后都没有终止... 代码要补全\n",
    "\n",
    "\n",
    "**参考：**\n",
    "\n",
    "1. [Run cells in different threads #1155](https://github.com/jupyter/notebook/issues/1155)\n",
    "\n",
    "2. [User friendly multiprocessing in Jupyter with ipywidgets](https://gist.github.com/micahscopes/2f523a8f485d3fe53cc32cef450ca27f)\n",
    "\n",
    "</br>\n",
    "\n",
    "## **2021-03-12**\n",
    "\n",
    "**看了一天终于意识到，参考代码是靠 QTimer 解决线程问题的，一直以为 qt 只是提供 GUI，结果竟是一个软件开发框架**\n",
    "\n",
    "</br>\n",
    "\n",
    "## **2021-03-11**\n",
    "\n",
    "**待解决问题：使用线程，并行执行任务**\n",
    "\n",
    "+ 单线程 beat tracking 脚本已能凑合运行，接下来需要把 stream 数据读取任务另开一个线程快速执行\n",
    "\n",
    "+ 学会线程的使用后，记得把前面实时绘制频谱的代码也再修改一下\n",
    "\n",
    "+ 通过 pyaudio callback 模式，对于 **callback** 函数的用法终于有些感觉了。不过 pyaudio 的 callback 模式不能用，因为我要分析两个频率范围的数据，而 callback 函数输入与返回的都是 1 组数据，所以还是用 block 模式，然后放在单独一个线程中去执行\n",
    "\n",
    "\n",
    "**傅立叶转换过程中需注意的几项操作：**\n",
    "\n",
    "不同代码中，fft 的处理流程基本不会完全一致，前几天也花了很多时间试图找到尽可能完整标准的流程，makerpotal 上音频相关的那几篇教程和代码看来最完整可靠，如参考5\n",
    "\n",
    "+ 从 pyaudio stream 中读取出的数据，做**标准化处理**\n",
    "\n",
    "+ 使用 **hann window**\n",
    "\n",
    "+ **amplitude scaling**，即 fft 之后的 y 值除以 N（参考3）\n",
    "\n",
    "+ **single sideband** 处理，即 fft 之后 y 值乘 2（参考1、2）\n",
    "\n",
    "\n",
    "**要耐心阅读官方文档**\n",
    "\n",
    "PortAudio 官方文档对该工具的功能以及 stream 等相关概念的介绍有助于理解音频处理流程与操作（参考10）；matlab 官网 audiotoolbox 的相关文档也很有帮助，特别是有几张框架图很好（参考8、9）\n",
    "\n",
    "一开始总是根据别人的代码学习使用某个新工具，但要用好还是得去认真看文档，不过先后顺序也确实不好死板规定，在阅读与实操往复中进步。\n",
    "\n",
    "\n",
    "**最近重度使用的就是 matplotlib，numpy 和 pyaudio 这几个库，只依赖少量的库做事的感觉很好，清楚自己到底在做什么**\n",
    "\n",
    "\n",
    "**傅立叶转换的数学公式还是应该花时间搞清楚，只有这样才能彻底理解**\n",
    "\n",
    "\n",
    "**参考：**\n",
    "\n",
    "1. [can someone explaing the computation for double sided and single sided spectrum in fft() example](https://www.mathworks.com/matlabcentral/answers/356376-can-someone-explaing-the-computation-for-double-sided-and-single-sided-spectrum-in-fft-example)\n",
    "\n",
    "2. [Breaking down confusions over Fast Fourier Transform (FFT)](https://medium.com/analytics-vidhya/breaking-down-confusions-over-fast-fourier-transform-fft-1561a029b1ab)\n",
    "\n",
    "3. [FFT: scaling for correct amplitude](https://ch.mathworks.com/matlabcentral/answers/545066-fft-scaling-for-correct-amplitude)\n",
    "\n",
    "4. [An Intro to Threading in Python](https://realpython.com/intro-to-python-threading/#using-a-threadpoolexecutor)\n",
    "\n",
    "5. [Recording Stereo Audio on a Raspberry Pi](https://makersportal.com/blog/recording-stereo-audio-on-a-raspberry-pi)\n",
    "\n",
    "6. [Audio Processing with The QuadMic 4-Microphone Array on the Raspberry Pi](https://makersportal.com/blog/audio-processing-with-the-quadmic-4-microphone-array-on-the-raspberry-pi?rq=audio)\n",
    "\n",
    "7. [Realtime FFT Audio Visualization with Python](https://swharden.com/blog/2013-05-09-realtime-fft-audio-visualization-with-python/)\n",
    "\n",
    "8. [Audio I/O: Buffering, Latency, and Throughput](https://in.mathworks.com/help/audio/gs/audio-io-buffering-latency-and-throughput.html)\n",
    "\n",
    "9. [Get Started with Audio Toolbox](https://in.mathworks.com/help/audio/gs/real-time-audio-in-matlab.html)\n",
    "\n",
    "10. [PortAudio API Overview ](http://files.portaudio.com/docs/v19-doxydocs/api_overview.html)\n",
    "\n",
    "</br>\n",
    "\n",
    "## **2021-03-08**\n",
    "\n",
    "**折腾几天之后决定还是暂时搁置对 fft 后的 y 做 logscale 转化（dB）**\n",
    "\n",
    "主要原因是：将声音的振幅转为响度需要一个参考值，即一个最大振幅 y，然后根据公式 20\\*log10（y/ymax)可以求得相对分贝值，即最大响度为 0dB，响度越小，负值越大。但是现在我没有好的办法找到一个绝对的最大响度值，即使通过试验手动找到一个最大值（如 900），为了方便可视化，得到的结果还需进一步转换，所以还是干脆先搁置，直接用 fft 后的振幅值即可，只要记得还可以做 log 转化就行。另外需要注意的是，有些示例代码中做分贝转换时，直接用的 y，而非 y/ymax 这样的比值，除非他们的 y 是标准化之后的，即 ymax=1，否则就是有问题的。另外，**麦克风的敏感性和校准** 也是暂时先不考虑但要记得的问题。\n",
    "\n",
    "**声音的响度**\n",
    "\n",
    "Intensity = Power/Area**\n",
    "\n",
    "Power: rate at which energy is transferred by wave\n",
    "\n",
    "Intensity = Amplitude squared/2 x MaterialDensity x SoundSpeed\n",
    "\n",
    "所以 Intensity 与波幅的平方正相关\n",
    "\n",
    "dB = 10log10(I/I0)\n",
    "\n",
    "注意 dB 反应的是一个比值，数值本身并没有实际意义\n",
    "\n",
    "I0 是参照强度，等于 10 的 -12 次方，当 I 也等于这个值时，dB 为0，每增大 10 倍，dB 值增加 10。\n",
    "\n",
    "**参考：**\n",
    "\n",
    "[Sound Intensity and Sound Level](https://courses.lumenlearning.com/physics/chapter/17-3-sound-intensity-and-sound-level/)\n",
    "\n",
    "[Decibels (dB) and Amplitude](https://blog.demofox.org/2015/04/14/decibels-db-and-amplitude/)\n",
    "\n",
    "[Recording Stereo Audio on a Raspberry Pi](https://makersportal.com/blog/recording-stereo-audio-on-a-raspberry-pi)\n",
    "\n",
    "[Audio Processing with The QuadMic 4-Microphone Array on the Raspberry Pi](https://makersportal.com/blog/audio-processing-with-the-quadmic-4-microphone-array-on-the-raspberry-pi)\n",
    "\n",
    "</br>\n",
    "\n",
    "## **2021-03-05**\n",
    "\n",
    "**今日问题：**\n",
    "\n",
    "**将 fft 之后的 y 转为 dB 并对应修改画图方式；麦克风需要检测与校准（未解决）**\n",
    "\n",
    "librosa load 音频文件时，默认 dtype='float32'，但是输出的 x 已经做了标准化，即除以2的31次方，所以是 float32 还是 float64 或 float16，并不会影响后续数据分析\n",
    "\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pyenv)",
   "language": "python",
   "name": "beatstracking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
